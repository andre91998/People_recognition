{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EA979 Final Project\n",
    "\n",
    "## Pedestrian Recognition: A HOG algorithm improved by frame dropping algorithm\n",
    "\n",
    "### Authors:\n",
    "Andr√© Barros de Medeiros\n",
    "\n",
    "Luca\n",
    "\n",
    "Francesco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by Andrew Rosebrock @ https://www.pyimagesearch.com/2015/11/09/pedestrian-detection-opencv/\n",
    "\n",
    "  OpenCV ships with a pre-trained HOG + Linear SVM model based, on Dalal and Triggs method to automatically detect pedestrians in images, that can be used to perform pedestrian detection in both images and video streams. Below we give an outline of the two:\n",
    "\n",
    "## Dependencies: \n",
    "    - OpenCV\n",
    "    - Numpy\n",
    "    - argparse\n",
    "    - imutils (for imutils in Anaconda: conda install -c conda-forge imutils)\n",
    "    - scipy.stats\n",
    "    - time\n",
    "  \n",
    "## Histogram of Oriented Gradients (HOG)\n",
    "\n",
    "While OpenCV's Cascade Classifiers are fast, they leave much to desire. That's where HOG comes in. But if you want to check out cascade classifiers, a java application for face recognition can be found [here](https://github.com/andre91998/JavaBasics/tree/master/FaceDetection).\n",
    "\n",
    "### HOG algorithm:\n",
    "\n",
    "<ul><li><strong>Step 1:</strong> Sample P positive samples from your training data of the object(s) you want to detect and extract HOG descriptors from these samples. </ul></li>\n",
    "\n",
    "<ul><li><strong>Step 2:</strong> Sample N negative samples from a negative training set that does not contain any of the objects you want to detect and extract HOG descriptors from these samples as well. In practice N >> P.</ul></li>\n",
    "\n",
    "<ul><li><strong>Step 3:</strong> Train a <strong>Linear Support Vector Machine</strong> on your positive and negative samples.</ul></li>\n",
    "\n",
    "<ul><li><strong>Step 4:</strong> Apply <em>hard-negative mining</em>. For each image and each possible scale of each image in your negative training set, apply the sliding window technique and slide your window across the image. At each window compute your HOG descriptors and apply your classifier. If your classifier (incorrectly) classifies a given window as an object (and it will, there will absolutely be false-positives), record the feature vector associated with the false-positive patch along with the probability of the classification. </ul></li>\n",
    "\n",
    "<ul><li><strong>Step 5:</strong> Take the false-positive samples found during the hard-negative mining stage, sort them by their confidence (i.e. probability) and re-train your classifier using these hard-negative samples. (Note: You can iteratively apply steps 4-5, but in practice one stage of hard-negative mining usually [not not always] tends to be enough. The gains in accuracy on subsequent runs of hard-negative mining tend to be minimal.)</ul></li>\n",
    "\n",
    "<ul><li><strong>Step 6: </strong>Your classifier is now trained and can be applied to your test dataset.</ul></li>\n",
    "\n",
    "## Linear Support Vector Machines (SVM)\n",
    "\n",
    "SVMs are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. More formally, a support-vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection.\n",
    "\n",
    "In the **linear** case (ours), the goal is to find the *\"maximum-margin hyperplane\"* that divides the data points into their correct classes, which means we want the distance between the closest data point of each class to be maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Idea:\n",
    "\n",
    "In this project, we use Pearson's Correlation Coeficient to evaluate how similar each frame is to the one before. This allowed us to establish criteria for dropping frames similiar to the previous, drastically lessening the amount of processing power needed (which in turn increased the speed of the algorithm).\n",
    "\n",
    "With PCC, each frame was attributed a value (coeficient). To decide wether or not to discard each frame, we compared the calculated value with a threshold value. If the coeficient was less than the threshold, than we re-applied the recognition algorithm, if not, then we just repeated the previously calculated identification rectangles (drawn around the identified people) on the new frame.\n",
    "\n",
    "On top of that, instead of setting a fixed threshold for every image, we decided to implement a threshold that would change through time. By establishing this dynamic threshold (through the updated mean of the PC coeficients), we further reduced the amount of frames to be processed, by dropping them when they are similar enough to the one before, maintaining a non-deterministic approach (and increasing accuracy and speed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9999999999999963, 0.0) 0.9999999999999963\n",
      "(0.9999908519433329, 0.0) 0.9999954259716646\n",
      "(0.9999649767325043, 0.0) 0.9999852762252779\n",
      "(0.999946229554237, 0.0) 0.9999755145575177\n",
      "(0.9999389095460005, 0.0) 0.9999681935552143\n",
      "(0.9999472285694058, 0.0) 0.9999646993909129\n",
      "(0.9999077154721001, 0.0) 0.9999565588310825\n",
      "(0.9999043878076271, 0.0) 0.9999500374531505\n",
      "(0.9998605896414907, 0.0) 0.9999400988074105\n",
      "(0.999928229275057, 0.0) 0.9999389118541752\n",
      "(0.9999404916163674, 0.0) 0.9999390554689199\n",
      "(0.9998962346052922, 0.0) 0.9999354870636177\n",
      "(0.9999429134036737, 0.0) 0.999936058320545\n",
      "(0.9998884001267396, 0.0) 0.9999326541638446\n",
      "(0.9999547526641501, 0.0) 0.9999341273971983\n",
      "(0.9996967049875547, 0.0) 0.9999192884965957\n",
      "(0.9998815057422822, 0.0) 0.9999170659816362\n",
      "(0.9999326856335535, 0.0) 0.999917933740076\n",
      "(0.9998498229584998, 0.0) 0.9999143489620983\n",
      "(0.9999472314471822, 0.0) 0.9999159930863526\n",
      "(0.9998942298662151, 0.0) 0.9999149567425365\n",
      "(0.9999532857647208, 0.0) 0.9999166989708176\n",
      "(0.9999100071360081, 0.0) 0.9999164080214781\n",
      "(0.9999585060206979, 0.0) 0.9999181621047789\n",
      "(0.9997996245728037, 0.0) 0.9999134206035\n",
      "(0.9999254092595322, 0.0) 0.999913881705655\n",
      "(0.9998763848042564, 0.0) 0.9999124929315292\n",
      "(0.9999313217557991, 0.0) 0.9999131653895388\n",
      "(0.9998732721321089, 0.0) 0.9999117897599723\n",
      "(0.9999524179441523, 0.0) 0.9999131440327782\n",
      "(0.9996633432474781, 0.0) 0.9999050859429298\n",
      "(0.9999201709937778, 0.0) 0.9999055573507687\n",
      "(0.9998549106104535, 0.0) 0.9999040226010621\n",
      "(0.9998993585910002, 0.0) 0.9999038854242956\n",
      "(0.9999596644043292, 0.0) 0.9999054791094395\n",
      "(0.9999227308841894, 0.0) 0.9999059583254046\n",
      "(0.9998810716382698, 0.0) 0.9999052857122389\n",
      "(0.99995376809467, 0.0) 0.9999065615644082\n",
      "(0.9999176894692178, 0.0) 0.9999068468953007\n",
      "(0.9998909824323888, 0.0) 0.9999064502837278\n",
      "(0.9999576344436125, 0.0) 0.9999076986778713\n",
      "(0.999920060361955, 0.0) 0.9999079930036829\n",
      "(0.9998917396298024, 0.0) 0.9999076150182439\n",
      "(0.9999463996582463, 0.0) 0.9999084964873348\n",
      "(0.9999024616375224, 0.0) 0.9999083623795613\n",
      "(0.9998477636902187, 0.0) 0.9999070450167495\n",
      "(0.9999349397182644, 0.0) 0.9999076385210371\n",
      "(0.9999043188487091, 0.0) 0.9999075693611967\n",
      "(0.9999414737990573, 0.0) 0.9999082612885\n",
      "(0.9999066726252411, 0.0) 0.9999082295152348\n",
      "(0.9999688159631261, 0.0) 0.9999094174848014\n",
      "(0.9999456178949143, 0.0) 0.9999101136465344\n",
      "(0.9999153191309331, 0.0) 0.9999102118632212\n",
      "(0.9998800830261748, 0.0) 0.9999096539217944\n",
      "(0.999964907758471, 0.0) 0.9999106585370067\n",
      "(0.999931460472013, 0.0) 0.9999110300001318\n",
      "(0.9999080511939155, 0.0) 0.9999109777403735\n",
      "(0.9999670206876483, 0.0) 0.9999119439980851\n",
      "(0.9999281518545515, 0.0) 0.9999122187075169\n",
      "(0.9999132497119891, 0.0) 0.9999122358909247\n",
      "(0.9978422007364587, 0.0) 0.9998783008883925\n",
      "(0.9999558976288585, 0.0) 0.9998795524487225\n",
      "(0.9998668978731349, 0.0) 0.9998793515824433\n",
      "(0.9999279549191646, 0.0) 0.9998801110095797\n",
      "(0.9996743929498136, 0.0) 0.9998769461163525\n",
      "(0.9999323966089749, 0.0) 0.9998777862753315\n",
      "(0.999756339431731, 0.0) 0.9998759736358748\n",
      "(0.9997016288956911, 0.0) 0.9998734097426367\n",
      "(0.9995729686297968, 0.0) 0.99986905552361\n",
      "(0.999872203209362, 0.0) 0.9998691004905493\n",
      "(0.9996546441732268, 0.0) 0.9998660799790379\n",
      "(0.9998589843949082, 0.0) 0.9998659814292582\n",
      "(0.9993494339246871, 0.0) 0.999858905436045\n",
      "(0.999689961593759, 0.0) 0.9998566224111493\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: Andre Barros de Medeiros\n",
    "@Date:29/30/2020\n",
    "@Copyright: Free to use, copy and modify\n",
    "\"\"\"\n",
    "\n",
    "# import the necessary packages\n",
    "from __future__ import print_function\n",
    "from collections import deque\n",
    "from imutils.object_detection import non_max_suppression\n",
    "from imutils.video import VideoStream\n",
    "from imutils import paths\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "\n",
    "#initialize frame counter\n",
    "counter = 0\n",
    "pts = deque(maxlen=32)\n",
    "\n",
    "coef = (0,0)\n",
    "\n",
    "vs = cv2.VideoCapture(\"/Users/Andre/Documents/GitHub/pedestrian_recognition/method4/pedestrian.mp4\")\n",
    "\n",
    "# allow the camera or video file to warm up\n",
    "time.sleep(2.0)\n",
    "\n",
    "# initialize the HOG descriptor/person detector\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "old_frame = None\n",
    "# keep looping\n",
    "while True:\n",
    "\n",
    "    # grab the current frame\n",
    "    frame = vs.read()\n",
    "    \n",
    "    # handle the frame from VideoCapture or VideoStream\n",
    "    frame = frame[1]\n",
    "    \n",
    "    # if we are viewing a video and we did not grab a frame,\n",
    "    # then we have reached the end of the video\n",
    "    if frame is None:\n",
    "        break\n",
    "    \n",
    "    if old_frame is None:\n",
    "        \n",
    "        # resize image it to (1) reduce detection time and (2) improve detection accuracy\n",
    "        frame = imutils.resize(frame, width=min(400, frame.shape[1]))\n",
    "        orig = frame.copy()\n",
    "        old_frame = orig.flatten() #update old frame\n",
    "\n",
    "        # detect people in the image\n",
    "        (rects, weights) = hog.detectMultiScale(frame, winStride=(7,7), \n",
    "                                                padding=(4,4), scale=1.3)\n",
    "        \n",
    "        # draw the original bounding boxes\n",
    "        for (x, y, w, h) in rects: \n",
    "            cv2.rectangle(orig, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        \n",
    "        # apply non-maxima suppression to the bounding boxes using a\n",
    "        # fairly large overlap threshold to try to maintain overlapping\n",
    "        # boxes that are still people\n",
    "        rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "        pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "        \n",
    "        # draw the final bounding boxes\n",
    "        for (xA, yA, xB, yB) in pick:\n",
    "        \t\tcv2.rectangle(frame, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "        \n",
    "        # show the frame\n",
    "        #cv2.imshow(\"Before NMS\", orig)\n",
    "        cv2.imshow(\"After NMS\", frame)\n",
    "        \n",
    "        # increment counter\n",
    "        counter += 1\n",
    "        \n",
    "        # if the 'q' key is pressed, stop the loop\n",
    "        key = cv2.waitKey(1) & 0xFF \n",
    "        # (& 0xFF) keeps last 8 bits of  waitKey output\n",
    "        if key == ord(\"q\"): break\n",
    "\n",
    "    if old_frame is not None:\n",
    "        \n",
    "        # resize image it to (1) reduce detection time and (2) improve detection accuracy\n",
    "        frame = imutils.resize(frame, width=min(400, frame.shape[1]))\n",
    "        \n",
    "        #flatten current frame for running the Pearson's Correlation \n",
    "        flat_frame = frame.flatten()\n",
    "        #calculate the pearson's correlation coeficient\n",
    "        coef = pearsonr(flat_frame, old_frame)\n",
    "        \n",
    "        #if on second frame, create threshold_arr for holding the PCCs\n",
    "        if (counter == 1): \n",
    "            threshold_arr = np.array(coef[0])\n",
    "            \n",
    "        #if on any other frame, append to the array\n",
    "        else:\n",
    "            threshold_arr = np.append(threshold_arr, coef[0])\n",
    "            \n",
    "        #dynamical threshold calcuation: mean of all previous PCCs    \n",
    "        threshold = np.mean(threshold_arr)\n",
    "        print(coef,threshold)\n",
    "        \n",
    "        #if PCC below the threshold, re-classify\n",
    "        if (((coef[0] < threshold)and(coef[0]>0))or((coef[0]>-1*threshold)and(coef[0]<0))):\n",
    "            orig = frame.copy()\n",
    "            old_frame = orig.flatten() #update old_frame\n",
    "            \n",
    "            # detect people in the image\n",
    "            (rects, weights) = hog.detectMultiScale(frame, winStride=(1,1), \n",
    "                                                    padding=(8,8), scale=1.5)\n",
    "            \n",
    "            # draw the original bounding boxes\n",
    "            for (x, y, w, h) in rects: \n",
    "                cv2.rectangle(orig, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "            \n",
    "            # apply non-maxima suppression to the bounding boxes using a\n",
    "            # fairly large overlap threshold to try to maintain overlapping\n",
    "            # boxes that are still people\n",
    "            rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "            pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "            \n",
    "            # draw the final bounding boxes\n",
    "            for (xA, yA, xB, yB) in pick:\n",
    "            \t\tcv2.rectangle(frame, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            \n",
    "            # show the frame\n",
    "            #cv2.imshow(\"Before NMS\", orig)\n",
    "            cv2.imshow(\"After NMS\", frame)\n",
    "            \n",
    "            # increment counter and update last frame\n",
    "            counter += 1\n",
    "            \n",
    "            # if the 'q' key is pressed, stop the loop\n",
    "            key = cv2.waitKey(1) & 0xFF \n",
    "            # (& 0xFF) keeps last 8 bits of  waitKey output\n",
    "            if key == ord(\"q\"): break\n",
    "        \n",
    "        # if PCC is above threshold, update frame, but keep same rectangles\n",
    "        else:\n",
    "            # draw the same rectangle as before on the current frame (which wasn't proccessed by HoG)\n",
    "            for (xA, yA, xB, yB) in pick:\n",
    "                cv2.rectangle(frame, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            \n",
    "            cv2.imshow(\"After NMS\", frame)\n",
    "            counter += 1\n",
    "            \n",
    "            # if the 'q' key is pressed, stop the loop\n",
    "            key = cv2.waitKey(1) & 0xFF \n",
    "            # (& 0xFF) keeps last 8 bits of  waitKey output\n",
    "            if key == ord(\"q\"): break\n",
    "            \n",
    "        \n",
    "# if we are not using a video file, stop the camera video stream\n",
    "if not args.get(\"video\", False): vs.stop()\n",
    "\n",
    "# otherwise, release the camera\n",
    "else: vs.release()\n",
    "\n",
    "# close all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
